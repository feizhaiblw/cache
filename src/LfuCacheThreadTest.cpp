#include "../include/LfuCache.h"
#include "LfuCache.cpp"
#include "../include/ThreadSafeTestFramework.h"
#include <iostream>
#include <cassert>
#include <thread>
#include <vector>
#include <chrono>
#include <random>
#include <mutex>
#include <shared_mutex>

/**
 * @brief LFUç¼“å­˜å¤šçº¿ç¨‹å®‰å…¨æµ‹è¯•
 * 
 * æ ¹æ®é¡¹ç›®è§„èŒƒï¼ŒLFUç¼“å­˜é»˜è®¤ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œéœ€è¦å¤–éƒ¨åŒæ­¥æªæ–½ã€‚
 * æœ¬æµ‹è¯•éªŒè¯åœ¨ä½¿ç”¨é€‚å½“åŒæ­¥æœºåˆ¶çš„æƒ…å†µä¸‹ï¼ŒLFUç¼“å­˜åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§ã€‚
 * LFUç­–ç•¥çš„ç‰¹ç‚¹æ˜¯æŒ‰è®¿é—®é¢‘ç‡æ·˜æ±°ï¼Œé¢‘ç‡ç›¸åŒæ—¶ä½¿ç”¨LRUä½œä¸ºtie-breakingç­–ç•¥ã€‚
 */

void testLfuBasicThreadSafety() {
    std::cout << "\n=== LFUç¼“å­˜åŸºæœ¬çº¿ç¨‹å®‰å…¨æµ‹è¯• ===" << std::endl;
    
    LfuCache<int, int> cache(100);
    ThreadSafeTestFramework<int, int> framework;
    std::shared_mutex cache_mutex;
    
    const int THREAD_COUNT = 4;
    const int OPERATIONS_PER_THREAD = 500;
    const int KEY_RANGE = 50;
    
    std::vector<std::thread> threads;
    
    // å¯åŠ¨å¤šä¸ªçº¿ç¨‹åŒæ—¶è¿›è¡ŒPUTæ“ä½œ
    for (int t = 0; t < THREAD_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &framework, t, OPERATIONS_PER_THREAD, KEY_RANGE]() {
            std::random_device rd;
            std::mt19937 gen(rd() + t);
            std::uniform_int_distribution<int> key_dist(0, KEY_RANGE - 1);
            
            for (int i = 0; i < OPERATIONS_PER_THREAD; ++i) {
                int key = key_dist(gen);
                int value = key * 100 + t * 10 + i; // ä¿è¯å”¯ä¸€æ€§
                
                ThreadSafeTestFramework<int, int>::Operation op(
                    ThreadSafeTestFramework<int, int>::OperationType::PUT, key, value);
                
                try {
                    std::unique_lock<std::shared_mutex> lock(cache_mutex);
                    cache.put(key, value);
                    op.success = true;
                    op.result = "PUTæˆåŠŸ";
                } catch (const std::exception& e) {
                    op.success = false;
                    op.result = std::string("PUTå¼‚å¸¸: ") + e.what();
                }
                
                framework.recordOperation(op);
            }
        });
    }
    
    framework.setStartTime();
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    framework.setEndTime();
    
    // éªŒè¯ç¼“å­˜çŠ¶æ€
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æµ‹è¯•åç¼“å­˜å¤§å°: " << cache.size() << "/" << cache.capacity() << std::endl;
        assert(cache.size() <= cache.capacity());
        assert(cache.size() >= 0);
    }
    
    framework.printStatistics();
    
    // æ•°æ®ä¸€è‡´æ€§éªŒè¯
    bool consistent = framework.validateDataConsistency(cache, KEY_RANGE);
    assert(consistent);
    
    std::cout << "âœ“ LFUåŸºæœ¬çº¿ç¨‹å®‰å…¨æµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuConcurrentReadWrite() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘è¯»å†™æµ‹è¯• ===" << std::endl;
    
    LfuCache<int, std::string> cache(50);
    ThreadSafeTestFramework<int, std::string> framework;
    std::shared_mutex cache_mutex;
    
    const int WRITER_COUNT = 2;
    const int READER_COUNT = 4;
    const int OPERATIONS_PER_THREAD = 300;
    const int KEY_RANGE = 30;
    
    // å…ˆå¡«å……ä¸€äº›åˆå§‹æ•°æ®
    {
        std::unique_lock<std::shared_mutex> lock(cache_mutex);
        for (int i = 0; i < 20; ++i) {
            cache.put(i, "initial_" + std::to_string(i));
        }
    }
    
    std::vector<std::thread> threads;
    std::atomic<bool> start_flag{false};
    
    // åˆ›å»ºå†™çº¿ç¨‹
    for (int t = 0; t < WRITER_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &framework, &start_flag, t, OPERATIONS_PER_THREAD, KEY_RANGE]() {
            // ç­‰å¾…å¼€å§‹ä¿¡å·
            while (!start_flag.load()) {
                std::this_thread::yield();
            }
            
            std::random_device rd;
            std::mt19937 gen(rd() + t);
            std::uniform_int_distribution<int> key_dist(0, KEY_RANGE - 1);
            
            for (int i = 0; i < OPERATIONS_PER_THREAD; ++i) {
                int key = key_dist(gen);
                std::string value = "writer_" + std::to_string(t) + "_" + std::to_string(i);
                
                ThreadSafeTestFramework<int, std::string>::Operation op(
                    ThreadSafeTestFramework<int, std::string>::OperationType::PUT, key, value);
                
                try {
                    std::unique_lock<std::shared_mutex> lock(cache_mutex);
                    cache.put(key, value);
                    op.success = true;
                    op.result = "WRITEæˆåŠŸ";
                } catch (const std::exception& e) {
                    op.success = false;
                    op.result = std::string("WRITEå¼‚å¸¸: ") + e.what();
                }
                
                framework.recordOperation(op);
                
                // éšæœºæš‚åœï¼Œå¢åŠ ç«äº‰
                if (i % 10 == 0) {
                    std::this_thread::sleep_for(std::chrono::microseconds(10));
                }
            }
        });
    }
    
    // åˆ›å»ºè¯»çº¿ç¨‹
    for (int t = 0; t < READER_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &framework, &start_flag, t, OPERATIONS_PER_THREAD, KEY_RANGE]() {
            // ç­‰å¾…å¼€å§‹ä¿¡å·
            while (!start_flag.load()) {
                std::this_thread::yield();
            }
            
            std::random_device rd;
            std::mt19937 gen(rd() + t + 100);
            std::uniform_int_distribution<int> key_dist(0, KEY_RANGE - 1);
            std::uniform_int_distribution<int> op_dist(0, 1); // 0=GET, 1=CONTAINS
            
            for (int i = 0; i < OPERATIONS_PER_THREAD; ++i) {
                int key = key_dist(gen);
                
                if (op_dist(gen) == 0) {
                    // GETæ“ä½œ - ä¼šå¢åŠ è®¿é—®é¢‘ç‡
                    ThreadSafeTestFramework<int, std::string>::Operation op(
                        ThreadSafeTestFramework<int, std::string>::OperationType::GET, key, "");
                    
                    try {
                        std::shared_lock<std::shared_mutex> lock(cache_mutex);
                        std::string value = cache.get(key);
                        op.success = true;
                        op.result = "READæˆåŠŸ: " + value;
                    } catch (const std::exception& e) {
                        op.success = false;
                        op.result = std::string("READå¼‚å¸¸: ") + e.what();
                    }
                    
                    framework.recordOperation(op);
                } else {
                    // CONTAINSæ“ä½œ - ä¸å½±å“è®¿é—®é¢‘ç‡
                    ThreadSafeTestFramework<int, std::string>::Operation op(
                        ThreadSafeTestFramework<int, std::string>::OperationType::CONTAINS, key, "");
                    
                    try {
                        std::shared_lock<std::shared_mutex> lock(cache_mutex);
                        bool exists = cache.contains(key);
                        op.success = true;
                        op.result = exists ? "CONTAINS:å­˜åœ¨" : "CONTAINS:ä¸å­˜åœ¨";
                    } catch (const std::exception& e) {
                        op.success = false;
                        op.result = std::string("CONTAINSå¼‚å¸¸: ") + e.what();
                    }
                    
                    framework.recordOperation(op);
                }
            }
        });
    }
    
    framework.setStartTime();
    start_flag.store(true); // å¼€å§‹æµ‹è¯•
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    framework.setEndTime();
    
    // éªŒè¯ç¼“å­˜çŠ¶æ€å’Œé¢‘ç‡ç»Ÿè®¡
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æµ‹è¯•åç¼“å­˜å¤§å°: " << cache.size() << "/" << cache.capacity() << std::endl;
        std::cout << "æœ€å°é¢‘ç‡: " << cache.getMinFrequency() << std::endl;
        assert(cache.size() <= cache.capacity());
    }
    
    framework.printStatistics();
    
    // éªŒè¯æ•°æ®ä¸€è‡´æ€§
    bool consistent = framework.validateDataConsistency(cache, KEY_RANGE);
    assert(consistent);
    
    std::cout << "âœ“ LFUå¹¶å‘è¯»å†™æµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuFrequencyTrackingUnderConcurrency() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘é¢‘ç‡è·Ÿè¸ªæµ‹è¯• ===" << std::endl;
    
    const int CACHE_CAPACITY = 10;
    LfuCache<int, std::string> cache(CACHE_CAPACITY);
    std::shared_mutex cache_mutex;
    
    // å…ˆæ’å…¥ä¸€äº›æ•°æ®
    {
        std::unique_lock<std::shared_mutex> lock(cache_mutex);
        for (int i = 0; i < 5; ++i) {
            cache.put(i, "value_" + std::to_string(i));
        }
    }
    
    const int THREAD_COUNT = 4;
    std::vector<std::thread> threads;
    std::atomic<bool> start_flag{false};
    std::atomic<int> total_gets{0};
    
    // åˆ›å»ºçº¿ç¨‹ï¼Œå¯¹ä¸åŒé”®è¿›è¡Œä¸åŒé¢‘ç‡çš„è®¿é—®
    for (int t = 0; t < THREAD_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &start_flag, &total_gets, t]() {
            while (!start_flag.load()) {
                std::this_thread::yield();
            }
            
            // ä¸åŒçº¿ç¨‹è®¿é—®ä¸åŒçš„é”®ï¼Œæ¨¡æ‹Ÿä¸åŒçš„è®¿é—®æ¨¡å¼
            int target_key = t % 5; // æ¯ä¸ªçº¿ç¨‹ä¸»è¦è®¿é—®ä¸€ä¸ªé”®
            int access_count = (t + 1) * 20; // ä¸åŒçº¿ç¨‹ä¸åŒçš„è®¿é—®æ¬¡æ•°
            
            for (int i = 0; i < access_count; ++i) {
                try {
                    std::shared_lock<std::shared_mutex> lock(cache_mutex);
                    if (cache.contains(target_key)) {
                        cache.get(target_key);
                        total_gets++;
                    }
                } catch (const std::exception&) {
                    // é”®å¯èƒ½ä¸å­˜åœ¨
                }
                
                std::this_thread::sleep_for(std::chrono::microseconds(50));
            }
        });
    }
    
    start_flag.store(true);
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    // éªŒè¯é¢‘ç‡è·Ÿè¸ªç»“æœ
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æ€»GETæ“ä½œæ¬¡æ•°: " << total_gets.load() << std::endl;
        std::cout << "æœ€å°é¢‘ç‡: " << cache.getMinFrequency() << std::endl;
        
        // æ£€æŸ¥ä¸åŒé”®çš„è®¿é—®é¢‘ç‡
        for (int i = 0; i < 5; ++i) {
            if (cache.contains(i)) {
                int frequency = cache.getFrequency(i);
                std::cout << "é”®" << i << "çš„è®¿é—®é¢‘ç‡: " << frequency << std::endl;
                assert(frequency >= 1); // è‡³å°‘æœ‰åˆå§‹æ’å…¥æ—¶çš„é¢‘ç‡
            }
        }
    }
    
    std::cout << "âœ“ LFUå¹¶å‘é¢‘ç‡è·Ÿè¸ªæµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuEvictionUnderConcurrency() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘æ·˜æ±°æœºåˆ¶æµ‹è¯• ===" << std::endl;
    
    const int CACHE_CAPACITY = 8;
    LfuCache<int, std::string> cache(CACHE_CAPACITY);
    std::shared_mutex cache_mutex;
    
    const int THREAD_COUNT = 3;
    const int OPERATIONS_PER_THREAD = 200;
    const int KEY_RANGE = 50; // è¿œå¤§äºç¼“å­˜å®¹é‡ï¼Œç¡®ä¿ä¼šå‘ç”Ÿæ·˜æ±°
    
    std::vector<std::thread> threads;
    std::atomic<int> total_puts{0};
    std::map<int, std::atomic<int>> key_access_counts;
    
    // åˆå§‹åŒ–è®¿é—®è®¡æ•°å™¨
    for (int i = 0; i < KEY_RANGE; ++i) {
        key_access_counts[i].store(0);
    }
    
    // å¤šçº¿ç¨‹å¹¶å‘æ’å…¥å’Œè®¿é—®ï¼Œæµ‹è¯•LFUæ·˜æ±°æœºåˆ¶
    for (int t = 0; t < THREAD_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &total_puts, &key_access_counts, t, OPERATIONS_PER_THREAD, KEY_RANGE]() {
            std::random_device rd;
            std::mt19937 gen(rd() + t);
            std::uniform_int_distribution<int> key_dist(0, KEY_RANGE - 1);
            std::uniform_int_distribution<int> op_dist(0, 2); // 0=PUT, 1=GET, 2=é‡å¤GET
            
            for (int i = 0; i < OPERATIONS_PER_THREAD; ++i) {
                int key = key_dist(gen);
                int operation = op_dist(gen);
                
                if (operation == 0) {
                    // PUTæ“ä½œ
                    std::string value = "thread_" + std::to_string(t) + "_op_" + std::to_string(i);
                    {
                        std::unique_lock<std::shared_mutex> lock(cache_mutex);
                        cache.put(key, value);
                        total_puts++;
                        key_access_counts[key]++; // PUTä¹Ÿç®—ä¸€æ¬¡è®¿é—®
                    }
                } else {
                    // GETæ“ä½œï¼ˆå¢åŠ è®¿é—®é¢‘ç‡ï¼‰
                    {
                        std::shared_lock<std::shared_mutex> lock(cache_mutex);
                        try {
                            cache.get(key);
                            key_access_counts[key]++;
                        } catch (const std::exception&) {
                            // é”®ä¸å­˜åœ¨æ˜¯æ­£å¸¸çš„
                        }
                    }
                }
                
                // å¯¹æŸäº›çƒ­ç‚¹é”®è¿›è¡Œé¢å¤–è®¿é—®
                if (key < 10 && i % 5 == 0) {
                    std::shared_lock<std::shared_mutex> lock(cache_mutex);
                    try {
                        cache.get(key);
                        key_access_counts[key]++;
                    } catch (const std::exception&) {
                        // é”®ä¸å­˜åœ¨
                    }
                }
            }
        });
    }
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    // éªŒè¯ç¼“å­˜å¤§å°å’Œæ·˜æ±°ç­–ç•¥æ•ˆæœ
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æ€»å…±æ‰§è¡ŒPUTæ“ä½œ: " << total_puts.load() << std::endl;
        std::cout << "æœ€ç»ˆç¼“å­˜å¤§å°: " << cache.size() << "/" << cache.capacity() << std::endl;
        std::cout << "æœ€å°é¢‘ç‡: " << cache.getMinFrequency() << std::endl;
        
        // éªŒè¯ç¼“å­˜ä¸­ä¿ç•™çš„æ˜¯é«˜é¢‘è®¿é—®çš„é”®
        std::cout << "ç¼“å­˜ä¸­ä¿ç•™çš„é”®åŠå…¶é¢‘ç‡: ";
        for (int i = 0; i < KEY_RANGE; ++i) {
            if (cache.contains(i)) {
                int frequency = cache.getFrequency(i);
                std::cout << i << "(" << frequency << ") ";
            }
        }
        std::cout << std::endl;
        
        assert(cache.size() <= CACHE_CAPACITY);
        assert(cache.size() >= 0);
        
        // ç”±äºå¤§é‡æ’å…¥ï¼Œç¼“å­˜åº”è¯¥æ¥è¿‘æ»¡å®¹é‡
        if (total_puts.load() > CACHE_CAPACITY) {
            assert(cache.size() == CACHE_CAPACITY);
        }
    }
    
    std::cout << "âœ“ LFUå¹¶å‘æ·˜æ±°æœºåˆ¶æµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuTieBreakingUnderConcurrency() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘Tie-breakingç­–ç•¥æµ‹è¯• ===" << std::endl;
    
    const int CACHE_CAPACITY = 5;
    LfuCache<int, std::string> cache(CACHE_CAPACITY);
    std::shared_mutex cache_mutex;
    
    // å…ˆæ’å…¥æ•°æ®ï¼Œä½¿æ‰€æœ‰é”®çš„é¢‘ç‡ç›¸ç­‰
    {
        std::unique_lock<std::shared_mutex> lock(cache_mutex);
        for (int i = 0; i < CACHE_CAPACITY; ++i) {
            cache.put(i, "equal_freq_" + std::to_string(i));
        }
        std::cout << "åˆå§‹å¡«å……ï¼Œæ‰€æœ‰é”®é¢‘ç‡ä¸º1" << std::endl;
    }
    
    const int THREAD_COUNT = 2;
    std::vector<std::thread> threads;
    std::atomic<bool> start_flag{false};
    
    // çº¿ç¨‹1ï¼šè®¿é—®æœ€æ—©æ’å…¥çš„é”®ï¼ˆé”®0ï¼‰
    threads.emplace_back([&cache, &cache_mutex, &start_flag]() {
        while (!start_flag.load()) {
            std::this_thread::yield();
        }
        
        std::this_thread::sleep_for(std::chrono::milliseconds(50)); // ç¨å¾®å»¶è¿Ÿ
        
        try {
            std::shared_lock<std::shared_mutex> lock(cache_mutex);
            if (cache.contains(0)) {
                cache.get(0); // è¿™ä¼šè®©é”®0çš„é¢‘ç‡å˜ä¸º2ï¼Œä½†æ›´æ–°è®¿é—®æ—¶é—´
            }
        } catch (const std::exception&) {
            // é”®å¯èƒ½ä¸å­˜åœ¨
        }
    });
    
    // çº¿ç¨‹2ï¼šæ’å…¥æ–°é”®ï¼Œè§¦å‘æ·˜æ±°
    threads.emplace_back([&cache, &cache_mutex, &start_flag]() {
        while (!start_flag.load()) {
            std::this_thread::yield();
        }
        
        std::this_thread::sleep_for(std::chrono::milliseconds(100)); // ç­‰å¾…çº¿ç¨‹1å®Œæˆ
        
        {
            std::unique_lock<std::shared_mutex> lock(cache_mutex);
            cache.put(10, "new_key"); // æ’å…¥æ–°é”®ï¼Œåº”è¯¥è§¦å‘æ·˜æ±°
        }
    });
    
    start_flag.store(true);
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    // éªŒè¯tie-breakingç­–ç•¥ï¼ˆé¢‘ç‡ç›¸åŒæ—¶ä½¿ç”¨LRUï¼‰
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æœ€ç»ˆç¼“å­˜å¤§å°: " << cache.size() << std::endl;
        std::cout << "ç¼“å­˜å†…å®¹: ";
        
        for (int i = 0; i <= 10; ++i) {
            if (cache.contains(i)) {
                int frequency = cache.getFrequency(i);
                std::cout << i << "(" << frequency << ") ";
            }
        }
        std::cout << std::endl;
        
        assert(cache.size() <= CACHE_CAPACITY);
        assert(cache.contains(10)); // æ–°æ’å…¥çš„é”®åº”è¯¥å­˜åœ¨
    }
    
    std::cout << "âœ“ LFUå¹¶å‘Tie-breakingç­–ç•¥æµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuConcurrentClearOperations() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘æ¸…ç©ºæµ‹è¯• ===" << std::endl;
    
    LfuCache<int, int> cache(100);
    std::shared_mutex cache_mutex;
    
    // å…ˆå¡«å……ç¼“å­˜
    {
        std::unique_lock<std::shared_mutex> lock(cache_mutex);
        for (int i = 0; i < 50; ++i) {
            cache.put(i, i * 10);
            // å¯¹ä¸€äº›é”®è¿›è¡Œé¢å¤–è®¿é—®ä»¥å»ºç«‹ä¸åŒé¢‘ç‡
            if (i < 10) {
                cache.get(i);
            }
        }
    }
    
    const int THREAD_COUNT = 4;
    std::vector<std::thread> threads;
    std::atomic<bool> start_flag{false};
    std::atomic<int> clear_count{0};
    std::atomic<int> operation_count{0};
    
    // åˆ›å»ºæµ‹è¯•çº¿ç¨‹ï¼šä¸€äº›çº¿ç¨‹è¿›è¡Œæ­£å¸¸æ“ä½œï¼Œä¸€äº›çº¿ç¨‹è¿›è¡Œæ¸…ç©ºæ“ä½œ
    for (int t = 0; t < THREAD_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &start_flag, &clear_count, &operation_count, t]() {
            while (!start_flag.load()) {
                std::this_thread::yield();
            }
            
            std::random_device rd;
            std::mt19937 gen(rd() + t);
            
            for (int i = 0; i < 80; ++i) {
                if (t == 0 && i % 30 == 0) {
                    // çº¿ç¨‹0è´Ÿè´£å¶å°”æ¸…ç©ºç¼“å­˜
                    std::unique_lock<std::shared_mutex> lock(cache_mutex);
                    cache.clear();
                    clear_count++;
                    std::cout << "çº¿ç¨‹" << t << "æ‰§è¡Œæ¸…ç©ºæ“ä½œï¼Œæ¬¡æ•°: " << clear_count.load() 
                              << "ï¼Œæœ€å°é¢‘ç‡é‡ç½®ä¸º: " << cache.getMinFrequency() << std::endl;
                } else {
                    // å…¶ä»–çº¿ç¨‹è¿›è¡Œæ­£å¸¸çš„PUT/GETæ“ä½œ
                    std::uniform_int_distribution<int> key_dist(0, 20);
                    int key = key_dist(gen);
                    
                    if (i % 2 == 0) {
                        // PUTæ“ä½œ
                        std::unique_lock<std::shared_mutex> lock(cache_mutex);
                        cache.put(key, key * 100 + t);
                    } else {
                        // GETæ“ä½œ
                        std::shared_lock<std::shared_mutex> lock(cache_mutex);
                        try {
                            cache.get(key);
                        } catch (const std::exception&) {
                            // æ­£å¸¸æƒ…å†µï¼Œç¼“å­˜å¯èƒ½è¢«æ¸…ç©ºäº†
                        }
                    }
                    operation_count++;
                }
                
                std::this_thread::sleep_for(std::chrono::microseconds(200));
            }
        });
    }
    
    start_flag.store(true);
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    std::cout << "æ¸…ç©ºæ“ä½œæ¬¡æ•°: " << clear_count.load() << std::endl;
    std::cout << "å…¶ä»–æ“ä½œæ¬¡æ•°: " << operation_count.load() << std::endl;
    
    // éªŒè¯æœ€ç»ˆçŠ¶æ€
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "æœ€ç»ˆç¼“å­˜å¤§å°: " << cache.size() << std::endl;
        std::cout << "æœ€ç»ˆæœ€å°é¢‘ç‡: " << cache.getMinFrequency() << std::endl;
        assert(cache.size() >= 0);
        assert(cache.size() <= cache.capacity());
    }
    
    std::cout << "âœ“ LFUå¹¶å‘æ¸…ç©ºæµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuStressTest() {
    std::cout << "\n=== LFUç¼“å­˜å‹åŠ›æµ‹è¯• ===" << std::endl;
    
    LfuCache<int, int> cache(200);
    ThreadSafeTestFramework<int, int> framework;
    
    // é«˜å¼ºåº¦å¹¶å‘æµ‹è¯•
    framework.mixedOperationsTest(cache, 6, 800, 80);
    
    framework.printStatistics();
    
    // éªŒè¯ç¼“å­˜çŠ¶æ€
    std::shared_mutex cache_mutex;
    {
        std::shared_lock<std::shared_mutex> lock(cache_mutex);
        std::cout << "å‹åŠ›æµ‹è¯•åç¼“å­˜å¤§å°: " << cache.size() << "/" << cache.capacity() << std::endl;
        std::cout << "æœ€å°é¢‘ç‡: " << cache.getMinFrequency() << std::endl;
        assert(cache.size() <= cache.capacity());
        assert(cache.size() >= 0);
    }
    
    std::cout << "âœ“ LFUå‹åŠ›æµ‹è¯•é€šè¿‡" << std::endl;
}

void testLfuExceptionsUnderConcurrency() {
    std::cout << "\n=== LFUç¼“å­˜å¹¶å‘å¼‚å¸¸å¤„ç†æµ‹è¯• ===" << std::endl;
    
    LfuCache<int, int> cache(10);
    std::shared_mutex cache_mutex;
    
    const int THREAD_COUNT = 4;
    const int OPERATIONS_PER_THREAD = 100;
    
    std::vector<std::thread> threads;
    std::atomic<int> exception_count{0};
    
    for (int t = 0; t < THREAD_COUNT; ++t) {
        threads.emplace_back([&cache, &cache_mutex, &exception_count, t, OPERATIONS_PER_THREAD]() {
            std::random_device rd;
            std::mt19937 gen(rd() + t);
            std::uniform_int_distribution<int> key_dist(100, 200); // ä½¿ç”¨ä¸å­˜åœ¨çš„é”®
            
            for (int i = 0; i < OPERATIONS_PER_THREAD; ++i) {
                int key = key_dist(gen);
                
                try {
                    std::shared_lock<std::shared_mutex> lock(cache_mutex);
                    cache.get(key); // å°è¯•è·å–ä¸å­˜åœ¨çš„é”®
                } catch (const std::out_of_range&) {
                    exception_count++;
                    // è¿™æ˜¯é¢„æœŸçš„å¼‚å¸¸
                } catch (const std::exception& e) {
                    std::cout << "æ„å¤–å¼‚å¸¸: " << e.what() << std::endl;
                }
            }
        });
    }
    
    for (auto& thread : threads) {
        thread.join();
    }
    
    std::cout << "æ•è·å¼‚å¸¸æ¬¡æ•°: " << exception_count.load() << std::endl;
    assert(exception_count.load() > 0); // åº”è¯¥æœ‰å¼‚å¸¸è¢«æ•è·
    
    std::cout << "âœ“ LFUå¹¶å‘å¼‚å¸¸å¤„ç†æµ‹è¯•é€šè¿‡" << std::endl;
}

int main() {
    try {
        std::cout << "å¼€å§‹LFUç¼“å­˜å¤šçº¿ç¨‹å®‰å…¨æµ‹è¯•..." << std::endl;
        std::cout << "æ³¨æ„: æ ¹æ®é¡¹ç›®è§„èŒƒï¼Œç¼“å­˜ç­–ç•¥é»˜è®¤ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œæœ¬æµ‹è¯•ä½¿ç”¨å¤–éƒ¨åŒæ­¥æªæ–½" << std::endl;
        std::cout << "LFUç‰¹ç‚¹: æŒ‰è®¿é—®é¢‘ç‡æ·˜æ±°ï¼Œé¢‘ç‡ç›¸åŒæ—¶ä½¿ç”¨LRUä½œä¸ºtie-breakingç­–ç•¥" << std::endl;
        
        testLfuBasicThreadSafety();
        testLfuConcurrentReadWrite();
        testLfuFrequencyTrackingUnderConcurrency();
        testLfuEvictionUnderConcurrency();
        testLfuTieBreakingUnderConcurrency();
        testLfuConcurrentClearOperations();
        testLfuStressTest();
        testLfuExceptionsUnderConcurrency();
        
        std::cout << "\nğŸ‰ æ‰€æœ‰LFUç¼“å­˜å¤šçº¿ç¨‹æµ‹è¯•é€šè¿‡ï¼" << std::endl;
        std::cout << "éªŒè¯äº†ä½¿ç”¨é€‚å½“å¤–éƒ¨åŒæ­¥æªæ–½æ—¶ï¼ŒLFUç¼“å­˜åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§ã€‚" << std::endl;
        std::cout << "ç¡®è®¤äº†LFUé¢‘ç‡è·Ÿè¸ªæœºåˆ¶å’Œtie-breakingç­–ç•¥åœ¨å¹¶å‘ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§ã€‚" << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ æµ‹è¯•å¤±è´¥: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}